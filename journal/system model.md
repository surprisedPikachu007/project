The process of fine-tuning a model for a specific task eliminates the necessity for a large model. This is particularly advantageous in scenarios where computational resources are constrained. For instance, tasks such as news article summarization can be effectively executed by a smaller model that has been fine-tuned for this specific purpose. In the context of this study, we utilize the Llama 2 foundational model, which is subsequently fine-tuned using the cnndailymail dataset. This dataset is comprises of over 300k news articles and their corresponding summaries. By using a QLoRA a Parameter efficient fine-tuning method, the 16 Gigabyte VRAM constraint is overcome. PeftModel is a base model class that is used to implement this and SFTTrainer short for Supervised Fine-tuning Trainer available in the Transformer Reinforcement Learning (TRL) library is where we pass the QLoRA configurations along with the training arguments. The bitsandbytes config class is what is going to enable us load the model in 4bit by setting the load_in_4bit flag. It is done by replacing the Linear layers with FP4/NF4 layers from bitsandbytes. A mini version of the dataset is utilized for the training process. This subset comprises 1,000 samples, randomly selected from the original dataset of over 300,000 samples. Example data is shown in Fig 3.1. This reduction in dataset size significantly decreases the computational load, thereby reducing the training time. Even when optimized techniques such as Parameter Efficient Fine-tuning and 4-bit quantization are employed, the training process still requires approximately one hour to complete on an Nvidia T4 GPU underscoring the computational intensity of fine-tuning large language models.

The deployment of the model can be approached in two ways. One option is to deploy the model as a real-time endpoint on a server, which allows for continuous inference and interaction. Alternatively, the model can be loaded into the Random Access Memory (RAM) for interaction within a notebook environment. In this study, we opt for the latter approach. This approach, while less scalable than server deployment, offers the advantage of simplicity and immediacy, making it an ideal choice for testing and demonstration purposes. It also allows for easy modification and experimentation with the model, which can be beneficial in the fine-tuning and optimization process. For user interaction a simple user interface is constructed using the Streamlit library. An input field for a search query is provided, which when submitted, is passed to the model for inference.
